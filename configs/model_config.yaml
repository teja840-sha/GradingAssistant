# LLM provider config (no secrets hardcoded).
# Values are read from environment variables (with safe defaults where applicable).
#
# Example: create a .env file (do NOT commit) with:
#   AZURE_OPENAI_API_KEY="..."
#   AZURE_OPENAI_ENDPOINT="https://your-endpoint.openai.azure.com/"
#   AZURE_OPENAI_DEPLOYMENT="gpt-4o"
#   AZURE_OPENAI_MODEL="gpt-4o"
#   AZURE_OPENAI_API_VERSION="2024-06-01"
#   AZURE_OPENAI_TEMPERATURE="0.0"
#   AZURE_OPENAI_TOP_P="1.0"
#   AZURE_OPENAI_SEED="1"
#
provider: autogen_ext.models.openai.AzureOpenAIChatCompletionClient
config:
  model: ${AZURE_OPENAI_MODEL:-gpt-4.1}
  api_key: ${AZURE_OPENAI_API_KEY}
  azure_endpoint: ${AZURE_OPENAI_ENDPOINT}
  azure_deployment: ${AZURE_OPENAI_DEPLOYMENT:-gpt-4.1}
  api_version: ${AZURE_OPENAI_API_VERSION:-2024-06-01}
  temperature: ${AZURE_OPENAI_TEMPERATURE:-0.0}
  top_p: ${AZURE_OPENAI_TOP_P:-1.0}
  seed: ${AZURE_OPENAI_SEED:-1}
